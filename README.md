# Administradores de Consorcios - CABA Scraper

Scraper del [buscador oficial de administradores de consorcios](https://buscador-admin-consorcio.buenosaires.gob.ar/administradores) de la Ciudad de Buenos Aires.

## üìã ¬øQu√© es esto?

Este proyecto extrae datos del Gobierno de la Ciudad de Buenos Aires (GCBA) sobre administradores de consorcios registrados, convirtiendo datos p√∫blicos en formato estructurado (CSV/Pandas DataFrame) para an√°lisis.

**Funcionalidad:**
- Extrae token CSRF din√°micamente del sitio
- Realiza b√∫squedas por matr√≠cula, CUIT, raz√≥n social, nombre, apellido, direcci√≥n
- Procesa respuestas JSON de la API interna del buscador
- Exporta resultados a CSV con formato limpio
- Arquitectura modular y simple (monol√≠tica)

**Casos de uso:**
- An√°lisis de mercado inmobiliario
- Verificaci√≥n de administradores activos
- Auditor√≠as de consorcios
- Investigaci√≥n acad√©mica sobre gesti√≥n de consorcios

## üöÄ Quick Start

```bash
# 1. Clonar repositorio
git clone https://github.com/PabloAlaniz/Administradores-Consorcios.git
cd Administradores-Consorcios

# 2. Crear entorno virtual (recomendado)
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Ejecutar scraper
python administradores_scraper.py

# 5. Ver resultados
cat administradores.csv
# O abrirlo en Excel/LibreOffice
```

**Resultado:** archivo `administradores.csv` con datos de administradores.

### Primera b√∫squeda personalizada

Editar `administradores_scraper.py` en la funci√≥n `build_post_data()`:

```python
def build_post_data(csrf_token, matricula='1234'):  # Cambiar matr√≠cula aqu√≠
    return {
        '_token': csrf_token,
        'matricula': matricula,  # Matr√≠cula a buscar
        'razonSocial': '',       # O buscar por raz√≥n social
        'nombre': '',            # O por nombre
        'apellido': '',          # O por apellido
        # ... m√°s filtros disponibles
    }
```

Luego ejecutar de nuevo:
```bash
python administradores_scraper.py
```

## üõ†Ô∏è Arquitectura

El proyecto incluye **dos versiones** del scraper:

### 1. `administradores_scraper.py` (Recomendado)

Versi√≥n refactorizada con funciones separadas para cada paso:

```python
# Flujo del scraper modular
csrf_token = get_csrf_token(url)              # 1. Obtener token
data = build_post_data(csrf_token)            # 2. Construir payload
headers = build_headers()                     # 3. Construir headers
json_data = fetch_administradores_data(...)   # 4. Fetch de datos
df = process_data_to_dataframe(json_data)     # 5. Procesar a DataFrame
filename = save_to_csv(df)                    # 6. Exportar CSV
```

**Ventajas:**
- ‚úÖ Funciones peque√±as y testeables
- ‚úÖ F√°cil de mantener y extender
- ‚úÖ Docstrings en todas las funciones
- ‚úÖ Separaci√≥n de responsabilidades

**Funciones principales:**
- `get_csrf_token(url)` ‚Äî Extrae token CSRF del HTML con BeautifulSoup
- `build_post_data(csrf_token, matricula)` ‚Äî Construye payload de b√∫squeda
- `build_headers()` ‚Äî Headers HTTP que emulan navegador m√≥vil
- `fetch_administradores_data(url, data, headers)` ‚Äî POST a API interna
- `process_data_to_dataframe(data)` ‚Äî Convierte JSON a DataFrame
- `save_to_csv(df, filename)` ‚Äî Exporta DataFrame a CSV UTF-8

### 2. `main.py` (Legacy)

Versi√≥n monol√≠tica original (todo en un archivo secuencial).

**Cu√°ndo usar:**
- Scripts r√°pidos one-off
- Debugging visual del flujo completo
- Referencia de implementaci√≥n simple

**‚ö†Ô∏è Recomendaci√≥n:** usar `administradores_scraper.py` para desarrollo activo.

### Estructura del proyecto

```
Administradores-Consorcios/
‚îú‚îÄ‚îÄ administradores_scraper.py  # Scraper modular (recomendado)
‚îú‚îÄ‚îÄ main.py                     # Scraper monol√≠tico (legacy)
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias
‚îú‚îÄ‚îÄ tests/                      # Tests unitarios
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_main.py
‚îú‚îÄ‚îÄ .coverage                   # Coverage report (gitignore)
‚îú‚îÄ‚îÄ administradores.csv         # Output generado (gitignore)
‚îî‚îÄ‚îÄ README.md
```

## üìä Output Format

El scraper genera un archivo `administradores.csv` con los siguientes campos (extra√≠dos del endpoint JSON del GCBA):

| Campo | Tipo | Descripci√≥n | Ejemplo |
|-------|------|-------------|---------|
| `MATRICULAID` | int | ID √∫nico de matr√≠cula | `3502` |
| `CUIT` | str | CUIT del administrador | `20-12345678-9` |
| `RAZONSOCIAL` | str | Raz√≥n social (empresa) | `Administraci√≥n S.A.` |
| `NOMBRE` | str | Nombre (persona f√≠sica) | `Juan` |
| `APELLIDO` | str | Apellido (persona f√≠sica) | `P√©rez` |
| `FECHAALTA` | str | Fecha de inscripci√≥n | `2020-03-15` |
| `DOMICILIOADMINISTRADOR` | str | Direcci√≥n completa | `Av. Corrientes 1234, CABA` |
| `TELEFONO` | str | Tel√©fono de contacto | `011-4567-8901` |
| `EMAIL` | str | Email de contacto | `admin@ejemplo.com` |
| `CANTIDADCONSORCIOS` | int | Consorcios administrados | `15` |

**Campos adicionales** (pueden variar seg√∫n la respuesta de la API):
- `BARRIO` ‚Äî Barrio del domicilio
- `CODIGOPOSTAL` ‚Äî C√≥digo postal
- `OBSERVACIONES` ‚Äî Notas adicionales

**Formato de export:**
- CSV UTF-8 sin BOM (compatible con Excel/LibreOffice)
- Header row incluida
- Sin index de pandas
- Separador: `,` (coma)

### Ejemplo de output

```csv
MATRICULAID,CUIT,RAZONSOCIAL,NOMBRE,APELLIDO,FECHAALTA,DOMICILIOADMINISTRADOR,CANTIDADCONSORCIOS
3502,20-12345678-9,Administraci√≥n S.A.,Juan,P√©rez,2020-03-15,Av. Corrientes 1234,15
3503,30-98765432-1,Consorcio Pro S.R.L.,Mar√≠a,Gonz√°lez,2019-07-22,Av. Santa Fe 5678,8
```

## üì¶ Dependencias

```bash
requests==2.31.0       # HTTP client (GET/POST a la API del GCBA)
beautifulsoup4==4.12.3 # Parsing HTML para extraer CSRF token
pandas==2.2.0          # Procesamiento de datos y export CSV
pytest==8.0.0          # Testing framework (dev dependency)
```

**Instalaci√≥n:**
```bash
pip install -r requirements.txt
```

**Sin entorno virtual:**
```bash
pip install requests beautifulsoup4 pandas
```

## üîß Configuraci√≥n

### Variables de entorno (opcional)

Por defecto, el scraper no requiere configuraci√≥n. Para avanzado:

```bash
export GCBA_BASE_URL="https://buscador-admin-consorcio.buenosaires.gob.ar"
export LOG_LEVEL="DEBUG"  # DEBUG | INFO | WARNING | ERROR
export OUTPUT_FILE="mi_archivo.csv"
```

**Nota:** actualmente el scraper usa valores hardcoded. Para usar env vars, modificar `administradores_scraper.py`.

### Personalizar b√∫squeda

Todos los filtros disponibles est√°n en `build_post_data()`:

```python
def build_post_data(csrf_token, matricula='3502'):
    return {
        '_token': csrf_token,
        'cuit': '',            # CUIT del administrador
        'matricula': matricula,# Matr√≠cula (requerido o vac√≠o)
        'tipo_filtro': '1',    # Tipo de b√∫squeda (1 = matr√≠cula)
        'razonSocial': '',     # Raz√≥n social (empresa)
        'nombre': '',          # Nombre (persona f√≠sica)
        'apellido': '',        # Apellido (persona f√≠sica)
        'calle': '',           # Calle del domicilio
        'altura': '',          # Altura de la calle
        'cuitConsorcio': '',   # CUIT del consorcio
        'isadmin': 'False'     # Es admin (siempre False)
    }
```

**B√∫squeda por raz√≥n social:**
```python
return {
    '_token': csrf_token,
    'matricula': '',
    'razonSocial': 'Administraci√≥n S.A.',
    # ... resto vac√≠o
}
```

**B√∫squeda por apellido:**
```python
return {
    '_token': csrf_token,
    'apellido': 'P√©rez',
    # ... resto vac√≠o
}
```

## üß™ Testing

El proyecto incluye tests unitarios con `pytest`:

```bash
# Ejecutar tests
pytest

# Con coverage
pytest --cov=. --cov-report=html

# Ver coverage report
open htmlcov/index.html
```

### Tests disponibles

```
tests/
‚îî‚îÄ‚îÄ test_main.py  # Tests del scraper monol√≠tico (main.py)
```

**TODO:**
- [ ] Tests para `administradores_scraper.py`
- [ ] Mocks de requests HTTP
- [ ] Tests de integraci√≥n con sitio real
- [ ] Coverage target: >80%

### Ejecutar tests manualmente

```python
# Test de extracci√≥n de CSRF token
from administradores_scraper import get_csrf_token
token = get_csrf_token('https://buscador-admin-consorcio.buenosaires.gob.ar/administradores')
print(f"Token: {token}")

# Test de construcci√≥n de headers
from administradores_scraper import build_headers
headers = build_headers()
assert 'User-Agent' in headers
assert 'X-Requested-With' in headers
```

## üîç C√≥mo funciona

### Paso 1: Extraer CSRF token

El sitio del GCBA usa protecci√≥n CSRF. Antes de hacer POST, necesitamos extraer el token del HTML:

```python
# GET a la p√°gina principal
response = requests.get(url)

# Parsear HTML con BeautifulSoup
soup = BeautifulSoup(response.text, 'html.parser')

# Extraer token de meta tag
csrf_token = soup.find('meta', {'name': 'csrf-token'})['content']
```

### Paso 2: Construir POST request

El formulario del buscador env√≠a datos v√≠a POST AJAX:

```python
data = {
    '_token': csrf_token,  # Token CSRF extra√≠do
    'matricula': '3502',   # Criterio de b√∫squeda
    # ... otros filtros
}

headers = {
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'X-Requested-With': 'XMLHttpRequest',  # AJAX request
    'User-Agent': 'Mozilla/5.0 ...',       # Emular navegador
    # ... otros headers
}
```

### Paso 3: Fetch de datos

```python
session = requests.Session()  # Mantiene cookies autom√°ticamente
response = session.post(url_post, data=data, headers=headers)
json_data = json.loads(response.text)
```

**Respuesta JSON del GCBA:**
```json
{
  "Objeto": [
    {
      "MATRICULAID": 3502,
      "CUIT": "20-12345678-9",
      "RAZONSOCIAL": "Administraci√≥n S.A.",
      // ... m√°s campos
    }
  ]
}
```

### Paso 4: Procesar y exportar

```python
# Convertir JSON a DataFrame
df = pd.json_normalize(json_data['Objeto'])

# Exportar a CSV UTF-8
df.to_csv('administradores.csv', index=False, encoding='utf-8')
```

## üêõ Troubleshooting

### Error: "ModuleNotFoundError: No module named 'pandas'"

**Soluci√≥n:**
```bash
pip install -r requirements.txt
```

Si persiste, instalar pandas manualmente:
```bash
pip install pandas
```

### Error: "KeyError: 'csrf-token'"

**Causa:** el sitio del GCBA cambi√≥ su estructura HTML.

**Soluci√≥n:**
1. Verificar que el sitio est√© accesible:
   ```bash
   curl https://buscador-admin-consorcio.buenosaires.gob.ar/administradores
   ```
2. Inspeccionar el HTML manualmente:
   ```python
   import requests
   from bs4 import BeautifulSoup
   
   response = requests.get('https://buscador-admin-consorcio.buenosaires.gob.ar/administradores')
   soup = BeautifulSoup(response.text, 'html.parser')
   print(soup.find('meta', {'name': 'csrf-token'}))
   ```
3. Ajustar el selector en `get_csrf_token()` si cambi√≥ la estructura.

### Error: "JSONDecodeError: Expecting value"

**Causa:** la API del GCBA retorn√≥ HTML de error en lugar de JSON.

**Soluci√≥n:**
1. Verificar respuesta HTTP:
   ```python
   print(f"Status Code: {response.status_code}")
   print(f"Response Text: {response.text[:500]}")  # Primeros 500 chars
   ```
2. Verificar que headers sean correctos (emular navegador).
3. El sitio puede estar bloqueando requests (rate limiting).

### CSV vac√≠o o sin resultados

**Causa:** la b√∫squeda no retorn√≥ resultados.

**Soluci√≥n:**
- Cambiar criterio de b√∫squeda (matr√≠cula, raz√≥n social, etc.)
- Verificar que la matr√≠cula exista:
  ```bash
  # Buscar matr√≠cula 1 (probablemente exista)
  # Editar build_post_data(csrf_token, matricula='1')
  python administradores_scraper.py
  ```

### Headers rechazan request (403/401)

**Causa:** headers no emulan correctamente un navegador.

**Soluci√≥n:**
1. Inspeccionar request real del navegador (DevTools ‚Üí Network ‚Üí Headers)
2. Actualizar `build_headers()` con valores actuales
3. Verificar que `Referer`, `Origin`, `User-Agent` coincidan

### Timeout en requests

**Soluci√≥n:**
```python
# En fetch_administradores_data(), agregar timeout
response = session.post(url_post, data=data, headers=headers, timeout=30)
```

## üíª Uso Avanzado

### B√∫squeda batch (m√∫ltiples matr√≠culas)

```python
from administradores_scraper import *

url_get = 'https://buscador-admin-consorcio.buenosaires.gob.ar/administradores'
url_post = url_get
csrf_token = get_csrf_token(url_get)

matriculas = ['1', '2', '3', '3502', '100']  # Lista de matr√≠culas
all_results = []

for mat in matriculas:
    data = build_post_data(csrf_token, matricula=mat)
    headers = build_headers()
    json_data = fetch_administradores_data(url_post, data, headers)
    df = process_data_to_dataframe(json_data)
    all_results.append(df)
    print(f"Matr√≠cula {mat}: {len(df)} resultados")

# Concatenar todos los DataFrames
import pandas as pd
final_df = pd.concat(all_results, ignore_index=True)
final_df.to_csv('administradores_batch.csv', index=False, encoding='utf-8')
print(f"Total: {len(final_df)} administradores")
```

### Exportar a Excel

```python
# Despu√©s de obtener el DataFrame
df.to_excel('administradores.xlsx', index=False, sheet_name='Administradores')
```

**Requiere:** `pip install openpyxl`

### Exportar a JSON

```python
# Exportar como JSON lines
df.to_json('administradores.jsonl', orient='records', lines=True, force_ascii=False)

# Exportar como JSON array
df.to_json('administradores.json', orient='records', force_ascii=False, indent=2)
```

### Rate limiting (buenas pr√°cticas)

```python
import time

for mat in matriculas:
    # ... fetch data ...
    time.sleep(2)  # Esperar 2 segundos entre requests
```

Esto evita sobrecargar el servidor del GCBA y posibles bloqueos.

## üìö Recursos

- **Sitio oficial:** [Buscador GCBA](https://buscador-admin-consorcio.buenosaires.gob.ar/administradores)
- **Documentaci√≥n BeautifulSoup:** [crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- **Documentaci√≥n Pandas:** [pandas.pydata.org](https://pandas.pydata.org/docs/)
- **Requests docs:** [requests.readthedocs.io](https://requests.readthedocs.io/)

## üìù Roadmap

- [x] Scraper funcional con b√∫squeda por matr√≠cula
- [x] Export a CSV
- [x] Refactorizaci√≥n modular
- [x] Docstrings en funciones
- [x] Tests b√°sicos
- [x] Actualizar `requirements.txt` (pandas incluido)
- [ ] CLI con argparse (b√∫squeda desde terminal)
- [ ] Manejo de errores robusto (reintentos, exponential backoff)
- [ ] Export a m√∫ltiples formatos (Excel, JSON, SQLite)
- [ ] B√∫squeda batch automatizada
- [ ] Coverage >80%
- [ ] GitHub Actions CI/CD
- [ ] Logging configurable (archivo + consola)
- [ ] Scraper as√≠ncrono (aiohttp)

## ‚ö†Ô∏è Disclaimer

Este proyecto es para **uso educativo y an√°lisis de datos p√∫blicos**.

**Responsabilidades:**
- ‚úÖ Los datos son p√∫blicos y accesibles en el sitio del GCBA
- ‚úÖ No se accede a informaci√≥n privada o protegida
- ‚ö†Ô∏è Respetar t√©rminos de uso del sitio oficial
- ‚ö†Ô∏è No hacer scraping masivo que afecte el servicio

**Uso bajo tu propia responsabilidad.**

## üìÑ Licencia

MIT License

Copyright (c) 2024 Pablo Alaniz

Se permite uso, copia, modificaci√≥n y distribuci√≥n con atribuci√≥n.

## ü§ù Contribuci√≥n

Contribuciones bienvenidas! Si ten√©s ideas o mejoras:

1. Fork el proyecto
2. Crea un branch (`git checkout -b feature/nueva-feature`)
3. Commit con convenciones (`git commit -m 'feat: nueva feature'`)
4. Push al branch (`git push origin feature/nueva-feature`)
5. Abre un Pull Request

### Reportar bugs

[Abr√≠ un issue](https://github.com/PabloAlaniz/Administradores-Consorcios/issues) con:
- Descripci√≥n del bug
- Pasos para reproducir
- Output esperado vs actual
- Versi√≥n de Python y dependencias

---

**Hecho por [@PabloAlaniz](https://github.com/PabloAlaniz)**  
**Repo:** https://github.com/PabloAlaniz/Administradores-Consorcios
